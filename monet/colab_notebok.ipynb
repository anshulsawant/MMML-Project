{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Check GPU\n",
    "import subprocess\n",
    "print(subprocess.check_output(['nvidia-smi'], text=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 2) Setup: deps, code, datasets, model\nimport shutil\nfrom pathlib import Path\nfrom huggingface_hub import snapshot_download\n\nWORK      = Path('/content/work/multimodal_ml')\nMONET_DIR = WORK / 'monet'\nVSTAR_DIR = WORK / 'datasets/vstar_bench'\nMME_DIR   = WORK / 'datasets/MME-RealWorld-Lite'\nGEO3K_DIR = WORK / 'datasets/geometry_3k'\nMODEL_DIR = Path('/content/work/models/Monet-7B')\nWORK.mkdir(parents=True, exist_ok=True)\n(WORK / 'datasets').mkdir(exist_ok=True)\n\n!apt-get -qq install -y git-lfs unzip && git lfs install\n!python -m pip install -q --no-cache-dir 'setuptools>=77' pillow\n!python -m pip uninstall -y -q torch torchvision torchaudio vllm transformers tokenizers || true\n!python -m pip install -q --no-cache-dir --index-url https://download.pytorch.org/whl/cu126 torch==2.7.1 torchvision==0.22.1 torchaudio==2.7.1\n!python -m pip install -q --no-cache-dir vllm==0.10.0 'transformers==4.56.1' qwen-vl-utils accelerate 'huggingface_hub>=0.25,<1.0' datasets==4.0.0\n\nif not MONET_DIR.exists():\n    !git clone --depth 1 https://github.com/NOVAglow646/Monet.git {MONET_DIR}\nif not VSTAR_DIR.exists():\n    !git clone --depth 1 https://huggingface.co/datasets/craigwu/vstar_bench {VSTAR_DIR}\nif not (GEO3K_DIR / 'test').exists():\n    !git clone --filter=blob:none --no-checkout --depth 1 https://github.com/lupantech/InterGPS /tmp/igps_tmp \\\n      && git -C /tmp/igps_tmp sparse-checkout set data/geometry3k \\\n      && git -C /tmp/igps_tmp checkout \\\n      && mv /tmp/igps_tmp/data/geometry3k {GEO3K_DIR} \\\n      && rm -rf /tmp/igps_tmp\n    for z in sorted(GEO3K_DIR.rglob('*.zip')): !unzip -oq {z} -d {z.parent}\nif not (MME_DIR / '.huggingface').exists():\n    if MME_DIR.exists(): shutil.rmtree(MME_DIR)\n    snapshot_download('yifanzhang114/MME-RealWorld-Lite', repo_type='dataset', local_dir=str(MME_DIR), local_dir_use_symlinks=False)\n    for z in sorted(MME_DIR.rglob('*.zip')): !unzip -oq {z} -d {z.parent}\nif not (MODEL_DIR / 'config.json').exists():\n    MODEL_DIR.mkdir(parents=True, exist_ok=True)\n    snapshot_download('NOVAglow646/Monet-7B', local_dir=str(MODEL_DIR), local_dir_use_symlinks=False)\n\nprint('Setup complete.')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Dataset sanity check (local files only, no inference)\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "WORK = Path('/content/work/multimodal_ml')\n",
    "DATASETS = WORK / 'datasets'\n",
    "VSTAR_DIR = DATASETS / 'vstar_bench'\n",
    "MME_DIR = DATASETS / 'MME-RealWorld-Lite'\n",
    "MME_DATA = MME_DIR / 'data'\n",
    "MME_IMGS = MME_DATA / 'imgs'\n",
    "GEO3K_DIR = DATASETS / 'geometry_3k'\n",
    "\n",
    "print('=== Dataset Roots ===')\n",
    "for p in [VSTAR_DIR, MME_DIR, GEO3K_DIR]:\n",
    "    print(f'{p} exists={p.exists()}')\n",
    "\n",
    "print()\n",
    "print('=== V* quick view ===')\n",
    "if VSTAR_DIR.exists():\n",
    "    for x in sorted(VSTAR_DIR.iterdir()):\n",
    "        print(x.name)\n",
    "    vstar_manifest = VSTAR_DIR / 'test_questions.jsonl'\n",
    "    if vstar_manifest.exists():\n",
    "        print()\n",
    "        print('V* test_questions.jsonl head:')\n",
    "        with open(vstar_manifest, 'r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i >= 3:\n",
    "                    break\n",
    "                print(line.rstrip())\n",
    "\n",
    "print()\n",
    "print('=== MME quick view ===')\n",
    "for p in [MME_DIR, MME_DATA, MME_IMGS]:\n",
    "    print(f'{p} exists={p.exists()}')\n",
    "\n",
    "ext_counts = Counter()\n",
    "for fp in MME_DIR.rglob('*'):\n",
    "    if fp.is_file():\n",
    "        ext = fp.suffix.lower() if fp.suffix else '(noext)'\n",
    "        ext_counts[ext] += 1\n",
    "print()\n",
    "print('MME file-type counts (top 20):')\n",
    "for ext, cnt in ext_counts.most_common(20):\n",
    "    print(f'{ext}: {cnt}')\n",
    "\n",
    "cand = []\n",
    "if MME_DATA.exists():\n",
    "    for ext in ('*.jsonl', '*.json', '*.parquet', '*.csv', '*.tsv'):\n",
    "        cand.extend(sorted(MME_DATA.rglob(ext)))\n",
    "\n",
    "print()\n",
    "print('MME candidate annotation files (first 20):')\n",
    "for fp in cand[:20]:\n",
    "    print('-', fp.relative_to(MME_DIR))\n",
    "\n",
    "ann = MME_DATA / 'MME-RealWorld-Lite.json'\n",
    "if ann.exists():\n",
    "    rows = json.load(open(ann, 'r', encoding='utf-8'))\n",
    "    print()\n",
    "    print(f'MME annotation rows: {len(rows)}')\n",
    "    if rows and isinstance(rows[0], dict):\n",
    "        print('MME first-row keys:', list(rows[0].keys()))\n",
    "\n",
    "    unresolved = []\n",
    "    preview = []\n",
    "    for r in rows[:20]:\n",
    "        img_val = str(r.get('Image', '')).strip()\n",
    "        p1 = Path(img_val)\n",
    "        if p1.is_absolute():\n",
    "            resolved = p1.exists()\n",
    "            target = p1\n",
    "        else:\n",
    "            cands = [MME_DATA / img_val, MME_IMGS / img_val, MME_IMGS / Path(img_val).name]\n",
    "            target = next((c for c in cands if c.exists()), cands[0])\n",
    "            resolved = any(c.exists() for c in cands)\n",
    "        if len(preview) < 5:\n",
    "            preview.append((img_val, str(target), resolved))\n",
    "        if not resolved:\n",
    "            unresolved.append(img_val)\n",
    "\n",
    "    print()\n",
    "    print('MME image resolution preview (first 5):')\n",
    "    for raw, target, ok in preview:\n",
    "        print(f'raw={raw} | resolved_path={target} | ok={ok}')\n",
    "    print(f'unresolved_in_first20={len(unresolved)}')\n",
    "else:\n",
    "    print()\n",
    "    print('WARNING: canonical annotation file not found:', ann)\n",
    "\n",
    "print()\n",
    "print('=== Geometry3K quick view ===')\n",
    "for split in ['train', 'val', 'test']:\n",
    "    d = GEO3K_DIR / split\n",
    "    if d.exists():\n",
    "        n = sum(1 for x in d.iterdir() if x.is_dir())\n",
    "        print(f'{split}: {n} problem folders')\n",
    "    else:\n",
    "        print(f'{split}: missing')\n",
    "\n",
    "print()\n",
    "print('Dataset sanity check complete.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 4) V*Bench — Monet-7B full test set evaluation\nimport importlib, json, os, re, sys, traceback\nfrom pathlib import Path\nfrom PIL import Image\nfrom transformers import AutoProcessor\n\nWORK          = Path('/content/work/multimodal_ml')\nMODEL_DIR     = Path('/content/work/models/Monet-7B')\nVSTAR_DIR     = WORK / 'datasets/vstar_bench'\nRESULTS_FILE  = WORK / 'results/vstar_results.json'\nMAX_MODEL_LEN = 32768\nSEED          = 0\nSAVE_EVERY    = 50\n\nos.environ.update({\n    'LATENT_SIZE': '10', 'LATENT_START_ID': '151666', 'LATENT_END_ID': '151667',\n    'VLLM_USE_V1': '1', 'VLLM_WORKER_MULTIPROC_METHOD': 'spawn', 'VLLM_ENABLE_V1_MULTIPROCESSING': '0',\n})\nRESULTS_FILE.parent.mkdir(parents=True, exist_ok=True)\n\nsys.path.insert(0, str(WORK / 'monet'))\nimport inference.apply_vllm_monet\nimport inference.load_and_gen_vllm as lg\n\n_f = WORK / 'monet/inference/load_and_gen_vllm.py'\n_src = _f.read_text()\n_src = _src.replace('enable_sleep_mode=True',     'enable_sleep_mode=False')\n_src = _src.replace('enable_chunked_prefill=True', 'enable_chunked_prefill=False')\n_f.write_text(_src)\nlg = importlib.reload(lg)\nlg.tqdm = lambda it, **kw: it\n\nfrom vllm import LLM\ntry:\n    _reuse = (isinstance(mllm, LLM)\n              and mllm.llm_engine.model_config.max_model_len == MAX_MODEL_LEN)\nexcept:\n    _reuse = False\n\nif not _reuse:\n    try:    mllm.shutdown()\n    except: pass\n    import subprocess\n    total_gib, free_gib = [float(x) / 1024 for x in subprocess.check_output(\n        'nvidia-smi --query-gpu=memory.total,memory.free --format=csv,noheader,nounits',\n        shell=True, text=True).strip().split(',')]\n    util = round(min(0.92, (free_gib - 4) / total_gib), 3)\n    mllm, sampling_params = lg.vllm_mllm_init(\n        str(MODEL_DIR), tp=1, gpu_memory_utilization=util, max_model_len=MAX_MODEL_LEN)\n    processor = AutoProcessor.from_pretrained(str(MODEL_DIR), trust_remote_code=True)\n\nsampling_params.seed = SEED\n\n# ── Dataset — resume from checkpoint if interrupted ───────────────────────\nall_rows = []\nwith open(VSTAR_DIR / 'test_questions.jsonl') as f:\n    for line in f:\n        if line.strip():\n            obj = json.loads(line)\n            if (VSTAR_DIR / obj['image']).exists():\n                all_rows.append(obj)\n\nresults = json.loads(RESULTS_FILE.read_text()) if RESULTS_FILE.exists() else []\ndone_ids = {r['question_id'] for r in results}\nremaining = [r for r in all_rows if r['question_id'] not in done_ids]\nprint(f'{len(all_rows)} total | {len(done_ids)} done | {len(remaining)} remaining')\n\n# ── Inference ─────────────────────────────────────────────────────────────\ndef extract_answer(text):\n    match = re.search(r'\\\\boxed\\{([^}]*)\\}', text)\n    if match and re.fullmatch(r'[A-Ea-e]', match.group(1).strip()):\n        return match.group(1).strip().upper()\n    letters = re.findall(r'\\b([A-E])\\b', text[-400:])\n    return letters[-1].upper() if letters else None\n\ndef clean_output(text):\n    \"\"\"Replace raw latent bytes between abs_vis_token tags with a readable placeholder.\"\"\"\n    return re.sub(r'(<abs_vis_token>)(.*?)(</abs_vis_token>)', r'\\1<latent>\\3', text, flags=re.DOTALL)\n\nfor i, row in enumerate(remaining, 1):\n    inputs = lg.vllm_mllm_process_batch_from_messages([[\n        {'role': 'user', 'content': [\n            {'type': 'text',  'text':  row['text']},\n            {'type': 'image', 'image': Image.open(VSTAR_DIR / row['image']).convert('RGB')},\n        ]},\n    ]], processor)\n\n    try:\n        output = mllm.generate(inputs, sampling_params=sampling_params, use_tqdm=False)[0].outputs[0]\n    except Exception:\n        print(f'[{i}/{len(remaining)}] ERROR {row[\"question_id\"]}:\\n{traceback.format_exc()}')\n        continue\n\n    token_ids    = list(getattr(output, 'token_ids', []) or [])\n    predicted    = extract_answer(output.text or '')\n    ground_truth = str(row.get('label', '')).strip().upper()\n    results.append({\n        'question_id':  row['question_id'],\n        'category':     row.get('category'),\n        'image':        row['image'],\n        'question':     row['text'],\n        'output':       clean_output(output.text or ''),\n        'predicted':    predicted,\n        'ground_truth': ground_truth,\n        'correct':      predicted == ground_truth,\n        'used_latent':  151666 in token_ids or 151667 in token_ids,\n        'finish':       output.finish_reason,\n        'tokens':       len(token_ids),\n    })\n\n    if i % SAVE_EVERY == 0 or i == len(remaining):\n        RESULTS_FILE.write_text(json.dumps(results, indent=2))\n        n       = len(results)\n        correct = sum(r['correct'] for r in results)\n        latent  = sum(r['used_latent'] for r in results)\n        print(f'[{i}/{len(remaining)}] saved={n} acc={correct/n:.3f} latent={latent/n:.3f}')\n\nn       = len(results)\ncorrect = sum(r['correct'] for r in results)\nlatent  = sum(r['used_latent'] for r in results)\nprint(f'\\n=== V*Bench Results (n={n}) ===')\nprint(f'accuracy:    {correct}/{n} = {correct/max(n,1):.3f}')\nprint(f'latent_rate: {latent}/{n} = {latent/max(n,1):.3f}')\nprint(f'results →    {RESULTS_FILE}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf215b0",
   "metadata": {},
   "outputs": [],
   "source": "# 5) MME-RealWorld-Lite — Monet-7B benchmark\nimport importlib, json, os, random, re, sys, traceback\nfrom collections import Counter\nfrom pathlib import Path\nfrom PIL import Image\nfrom transformers import AutoProcessor\n\nWORK          = Path('/content/work/multimodal_ml')\nMODEL_DIR     = Path('/content/work/models/Monet-7B')\nMME_DIR       = WORK / 'datasets/MME-RealWorld-Lite'\nN_SAMPLE      = 20\nSEED          = 0\nMAX_MODEL_LEN = 32768\n\nos.environ.update({\n    'LATENT_SIZE': '10', 'LATENT_START_ID': '151666', 'LATENT_END_ID': '151667',\n    'VLLM_USE_V1': '1', 'VLLM_WORKER_MULTIPROC_METHOD': 'spawn', 'VLLM_ENABLE_V1_MULTIPROCESSING': '0',\n})\nrandom.seed(SEED)\n\n# ── Monet inference module ────────────────────────────────────────────────\nsys.path.insert(0, str(WORK / 'monet'))\nimport inference.apply_vllm_monet\nimport inference.load_and_gen_vllm as lg\n\n_f = WORK / 'monet/inference/load_and_gen_vllm.py'\n_src = _f.read_text()\n_src = _src.replace('enable_sleep_mode=True',     'enable_sleep_mode=False')\n_src = _src.replace('enable_chunked_prefill=True', 'enable_chunked_prefill=False')\n_f.write_text(_src)\nlg = importlib.reload(lg)\nlg.tqdm = lambda it, **kw: it\n\n# ── Engine — reinit if config changed, otherwise reuse ───────────────────\nfrom vllm import LLM\ntry:\n    _reuse = (isinstance(mllm, LLM)\n              and mllm.llm_engine.model_config.max_model_len == MAX_MODEL_LEN)\nexcept:\n    _reuse = False\n\nif not _reuse:\n    try:    mllm.shutdown()\n    except: pass\n    import subprocess\n    total_gib, free_gib = [float(x) / 1024 for x in subprocess.check_output(\n        'nvidia-smi --query-gpu=memory.total,memory.free --format=csv,noheader,nounits',\n        shell=True, text=True).strip().split(',')]\n    util = round(min(0.92, (free_gib - 4) / total_gib), 3)\n    mllm, sampling_params = lg.vllm_mllm_init(\n        str(MODEL_DIR), tp=1, gpu_memory_utilization=util, max_model_len=MAX_MODEL_LEN)\n    processor = AutoProcessor.from_pretrained(str(MODEL_DIR), trust_remote_code=True)\n\nsampling_params.seed = SEED\n\n# ── Dataset ───────────────────────────────────────────────────────────────\nrows = json.load(open(MME_DIR / 'data/MME-RealWorld-Lite.json'))\nprint(f'{len(rows)} annotations | keys: {list(rows[0].keys())}')\n\nimage_index = {\n    f.name: f for f in (MME_DIR / 'data').rglob('*')\n    if f.suffix.lower() in {'.jpg', '.jpeg', '.png', '.webp', '.bmp'}\n}\n\nsamples = random.sample(rows, min(N_SAMPLE, len(rows)))\n\n# ── Eval ──────────────────────────────────────────────────────────────────\ndef extract_answer(text):\n    match = re.search(r'\\\\boxed\\{([^}]*)\\}', text)\n    if match and re.fullmatch(r'[A-Ea-e]', match.group(1).strip()):\n        return match.group(1).strip().upper()\n    letters = re.findall(r'\\b([A-E])\\b', text[-400:])\n    return letters[-1].upper() if letters else None\n\nfinish_counts = Counter()\nskip_counts   = Counter()\nnum_correct   = 0\nnum_latent    = 0\n\nfor i, row in enumerate(samples, 1):\n    image_path = image_index.get(Path(row['Image']).name)\n    if image_path is None:\n        skip_counts['no_image'] += 1\n        print(f'[{i:02d}] SKIP (not found): {row[\"Image\"]}')\n        continue\n\n    question     = row['Text']\n    ground_truth = row['Ground truth'].strip().upper()\n    choices      = row.get('Answer choices', [])\n    if choices:\n        question += '\\n' + '\\n'.join(f'({chr(65+j)}) {c}' for j, c in enumerate(choices))\n    question += '\\nPut your final answer in \\\\boxed{}.'\n\n    inputs = lg.vllm_mllm_process_batch_from_messages([[\n        {'role': 'user', 'content': [{'type': 'text', 'text': question},\n                                     {'type': 'image', 'image': Image.open(image_path).convert('RGB')}]},\n    ]], processor)\n\n    try:\n        output = mllm.generate(inputs, sampling_params=sampling_params, use_tqdm=False)[0].outputs[0]\n    except Exception:\n        skip_counts['error'] += 1\n        print(f'[{i:02d}] ERROR:\\n{traceback.format_exc()}')\n        continue\n\n    raw_text    = output.text or ''\n    predicted   = extract_answer(raw_text)\n    token_ids   = list(getattr(output, 'token_ids', []) or [])\n    correct     = (predicted == ground_truth) if ground_truth else False\n    used_latent = 151666 in token_ids or 151667 in token_ids\n\n    finish_counts[str(output.finish_reason)] += 1\n    num_correct += correct\n    num_latent  += used_latent\n\n    tail = raw_text.replace('\\n', ' ').strip()[-300:]\n    print(f'[{i:02d}] correct={correct} latent={used_latent} pred={predicted} gt={ground_truth} finish={output.finish_reason} tokens={len(token_ids)}')\n    print(f'     ...{tail}')\n\nprocessed = len(samples) - sum(skip_counts.values())\nprint(f'\\n=== Results (n={processed}) ===')\nprint(f'accuracy:    {num_correct}/{processed} = {num_correct / max(processed, 1):.3f}')\nprint(f'latent_rate: {num_latent}/{processed} = {num_latent / max(processed, 1):.3f}')\nprint(f'finish:      {dict(finish_counts)}')\nif skip_counts:\n    print(f'skipped:     {dict(skip_counts)}')"
  },
  {
   "cell_type": "code",
   "id": "hsynq4boy0m",
   "source": "# 6) Cleanup — free GPU memory\n# Note: vLLM holds a CUDA private pool; full release requires a runtime restart.\n# This does best-effort cleanup for repeated runs within the same session.\nimport gc, torch\n\ntry:    mllm.shutdown()\nexcept: pass\ntry:    del mllm, sampling_params, processor\nexcept: pass\n\ngc.collect()\ntorch.cuda.synchronize()\ntorch.cuda.empty_cache()\n!pkill -f vllm || true\n!pkill -f ray  || true\n\nprint('Cleanup done. For full GPU release, restart the runtime.')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "nxwblr0ka0a",
   "source": "# 7) Repo inference example — vllm_inference_example.py reproduced in-notebook\n# Runs the exact conversation from monet/inference/vllm_inference_example.py.\nimport importlib, os, re, subprocess, sys\nfrom pathlib import Path\nfrom PIL import Image\nfrom transformers import AutoProcessor\n\nWORK      = Path('/content/work/multimodal_ml')\nMODEL_DIR = Path('/content/work/models/Monet-7B')\n\nos.environ.update({\n    'LATENT_SIZE': '10', 'LATENT_START_ID': '151666', 'LATENT_END_ID': '151667',\n    'VLLM_USE_V1': '1', 'VLLM_WORKER_MULTIPROC_METHOD': 'spawn', 'VLLM_ENABLE_V1_MULTIPROCESSING': '0',\n})\n\nsys.path.insert(0, str(WORK / 'monet'))\nimport inference.apply_vllm_monet\nimport inference.load_and_gen_vllm as lg\n\n_f = WORK / 'monet/inference/load_and_gen_vllm.py'\n_src = _f.read_text()\n_src = _src.replace('enable_sleep_mode=True',      'enable_sleep_mode=False')\n_src = _src.replace('enable_chunked_prefill=True',  'enable_chunked_prefill=False')\n_f.write_text(_src)\nlg = importlib.reload(lg)\nlg.tqdm = lambda it, **kw: it\n\nfrom vllm import LLM\ntry:\n    _reuse = isinstance(mllm, LLM)\nexcept NameError:\n    _reuse = False\n\nif not _reuse:\n    total_gib, free_gib = [float(x) / 1024 for x in subprocess.check_output(\n        'nvidia-smi --query-gpu=memory.total,memory.free --format=csv,noheader,nounits',\n        shell=True, text=True).strip().split(',')]\n    util = round(min(0.92, (free_gib - 4) / total_gib), 3)\n    mllm, sampling_params = lg.vllm_mllm_init(str(MODEL_DIR), tp=1, gpu_memory_utilization=util)\n    processor = AutoProcessor.from_pretrained(str(MODEL_DIR), trust_remote_code=True)\n\n# ── Verbatim from vllm_inference_example.py ──────────────────────────────\ndef replace_abs_vis_token_content(s: str) -> str:\n    pattern = re.compile(r'(<abs_vis_token>)(.*?)(</abs_vis_token>)', flags=re.DOTALL)\n    return pattern.sub(r'\\1<latent>\\3', s)\n\nquestion = (\n    'Question:  Which car has the longest rental period? '\n    'The choices are listed below:\\n'\n    '(A)DB11 COUPE.\\n'\n    '(B) V12 VANTAGES COUPES.\\n'\n    '(C) VANQUISH VOLANTE.\\n'\n    '(D) V12 VOLANTE.\\n'\n    '(E) The image does not feature the time. '\n    'Put your final answer in \\\\boxed{}.'\n)\nimage = Image.open(WORK / 'monet/images/example_question.png').convert('RGB')\n\ninputs = lg.vllm_mllm_process_batch_from_messages([[\n    {'role': 'user', 'content': [{'type': 'text', 'text': question},\n                                 {'type': 'image', 'image': image}]},\n]], processor)\n\noutput    = mllm.generate(inputs, sampling_params=sampling_params, use_tqdm=False)[0].outputs[0]\ntoken_ids = list(getattr(output, 'token_ids', []) or [])\nlatent    = 151666 in token_ids or 151667 in token_ids\n\nprint(f'finish={output.finish_reason}  tokens={len(token_ids)}  latent={latent}')\nprint(replace_abs_vis_token_content(output.text or ''))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "0wmez6llwdhc",
   "source": "# 8) Geometry3K — Monet-7B full test set evaluation\nimport importlib, json, os, re, subprocess, sys, traceback\nfrom pathlib import Path\nfrom PIL import Image\nfrom transformers import AutoProcessor\n\nWORK          = Path('/content/work/multimodal_ml')\nMODEL_DIR     = Path('/content/work/models/Monet-7B')\nGEO3K_DIR     = WORK / 'datasets/geometry_3k/test'\nRESULTS_FILE  = WORK / 'results/geo3k_results.json'\nMAX_MODEL_LEN = 32768\nSEED          = 0\nSAVE_EVERY    = 50\n\nos.environ.update({\n    'LATENT_SIZE': '10', 'LATENT_START_ID': '151666', 'LATENT_END_ID': '151667',\n    'VLLM_USE_V1': '1', 'VLLM_WORKER_MULTIPROC_METHOD': 'spawn', 'VLLM_ENABLE_V1_MULTIPROCESSING': '0',\n})\nRESULTS_FILE.parent.mkdir(parents=True, exist_ok=True)\n\nsys.path.insert(0, str(WORK / 'monet'))\nimport inference.apply_vllm_monet\nimport inference.load_and_gen_vllm as lg\n\n_f = WORK / 'monet/inference/load_and_gen_vllm.py'\n_src = _f.read_text()\n_src = _src.replace('enable_sleep_mode=True',     'enable_sleep_mode=False')\n_src = _src.replace('enable_chunked_prefill=True', 'enable_chunked_prefill=False')\n_f.write_text(_src)\nlg = importlib.reload(lg)\nlg.tqdm = lambda it, **kw: it\n\nfrom vllm import LLM\ntry:\n    _reuse = (isinstance(mllm, LLM)\n              and mllm.llm_engine.model_config.max_model_len == MAX_MODEL_LEN)\nexcept:\n    _reuse = False\n\nif not _reuse:\n    try:    mllm.shutdown()\n    except: pass\n    total_gib, free_gib = [float(x) / 1024 for x in subprocess.check_output(\n        'nvidia-smi --query-gpu=memory.total,memory.free --format=csv,noheader,nounits',\n        shell=True, text=True).strip().split(',')]\n    util = round(min(0.92, (free_gib - 4) / total_gib), 3)\n    mllm, sampling_params = lg.vllm_mllm_init(\n        str(MODEL_DIR), tp=1, gpu_memory_utilization=util, max_model_len=MAX_MODEL_LEN)\n    processor = AutoProcessor.from_pretrained(str(MODEL_DIR), trust_remote_code=True)\n\nsampling_params.seed = SEED\n\n# ── Dataset — resume from checkpoint if interrupted ───────────────────────\nproblem_dirs = sorted(GEO3K_DIR.iterdir(), key=lambda p: int(p.name))\n\nresults  = json.loads(RESULTS_FILE.read_text()) if RESULTS_FILE.exists() else []\ndone_ids = {r['id'] for r in results}\nremaining = [d for d in problem_dirs if str(d.name) not in done_ids]\nprint(f'{len(problem_dirs)} total | {len(done_ids)} done | {len(remaining)} remaining')\n\n# ── Helpers ───────────────────────────────────────────────────────────────\ndef build_prompt(problem_text, choices):\n    lines = [f'Question: {problem_text}', 'Choices:']\n    for i, c in enumerate(choices):\n        lines.append(f'({chr(65+i)}) {c}')\n    lines.append('Put your final answer in \\\\boxed{}.')\n    return '\\n'.join(lines)\n\ndef extract_answer(text, choices):\n    m = re.search(r'\\\\boxed\\{([^}]*)\\}', text)\n    if m:\n        boxed = m.group(1).strip()\n        if re.fullmatch(r'[A-Da-d]', boxed):\n            return boxed.upper()\n        norm = lambda s: re.sub(r'\\s+', ' ', s.strip().upper())\n        for i, c in enumerate(choices):\n            if norm(boxed) == norm(c):\n                return chr(65 + i)\n    letters = re.findall(r'\\b([A-D])\\b', text[-400:])\n    return letters[-1].upper() if letters else None\n\ndef clean_output(text):\n    return re.sub(r'(<abs_vis_token>)(.*?)(</abs_vis_token>)', r'\\1<latent>\\3', text, flags=re.DOTALL)\n\n# ── Inference ─────────────────────────────────────────────────────────────\nfor i, prob_dir in enumerate(remaining, 1):\n    data     = json.loads((prob_dir / 'data.json').read_text())\n    question = build_prompt(data['problem_text'], data['choices'])\n    try:\n        image  = Image.open(prob_dir / 'img_diagram.png').convert('RGB')\n        inputs = lg.vllm_mllm_process_batch_from_messages([[\n            {'role': 'user', 'content': [\n                {'type': 'text',  'text':  question},\n                {'type': 'image', 'image': image},\n            ]},\n        ]], processor)\n        output = mllm.generate(inputs, sampling_params=sampling_params, use_tqdm=False)[0].outputs[0]\n    except Exception:\n        print(f'[{i}/{len(remaining)}] ERROR {prob_dir.name}:\\n{traceback.format_exc()}')\n        continue\n\n    token_ids    = list(getattr(output, 'token_ids', []) or [])\n    predicted    = extract_answer(output.text or '', data['choices'])\n    ground_truth = str(data['answer']).strip().upper()\n    results.append({\n        'id':           str(prob_dir.name),\n        'problem_type': data.get('problem_type_graph', []),\n        'goal_type':    data.get('problem_type_goal', []),\n        'question':     question,\n        'output':       clean_output(output.text or ''),\n        'predicted':    predicted,\n        'ground_truth': ground_truth,\n        'correct':      predicted == ground_truth,\n        'used_latent':  151666 in token_ids or 151667 in token_ids,\n        'finish':       output.finish_reason,\n        'tokens':       len(token_ids),\n    })\n\n    if i % SAVE_EVERY == 0 or i == len(remaining):\n        RESULTS_FILE.write_text(json.dumps(results, indent=2))\n        n       = len(results)\n        correct = sum(r['correct'] for r in results)\n        latent  = sum(r['used_latent'] for r in results)\n        print(f'[{i}/{len(remaining)}] saved={n} acc={correct/n:.3f} latent={latent/n:.3f}')\n\nn       = len(results)\ncorrect = sum(r['correct'] for r in results)\nlatent  = sum(r['used_latent'] for r in results)\nprint(f'\\n=== Geometry3K Results (n={n}) ===')\nprint(f'accuracy:    {correct}/{n} = {correct/max(n,1):.3f}')\nprint(f'latent_rate: {latent}/{n} = {latent/max(n,1):.3f}')\nprint(f'results →    {RESULTS_FILE}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "9igsl7q7sdi",
   "source": "# 9) Geometry3K — text-only baseline (question + choices, no image)\nimport importlib, json, os, re, subprocess, sys, traceback\nfrom pathlib import Path\nfrom transformers import AutoProcessor\n\nWORK          = Path('/content/work/multimodal_ml')\nMODEL_DIR     = Path('/content/work/models/Monet-7B')\nGEO3K_DIR     = WORK / 'datasets/geometry_3k/test'\nRESULTS_FILE  = WORK / 'results/geo3k_results_no_image.json'\nMAX_MODEL_LEN = 32768\nSEED          = 0\nSAVE_EVERY    = 50\n\nos.environ.update({\n    'LATENT_SIZE': '10', 'LATENT_START_ID': '151666', 'LATENT_END_ID': '151667',\n    'VLLM_USE_V1': '1', 'VLLM_WORKER_MULTIPROC_METHOD': 'spawn', 'VLLM_ENABLE_V1_MULTIPROCESSING': '0',\n})\nRESULTS_FILE.parent.mkdir(parents=True, exist_ok=True)\n\nsys.path.insert(0, str(WORK / 'monet'))\nimport inference.apply_vllm_monet\nimport inference.load_and_gen_vllm as lg\n\n_f = WORK / 'monet/inference/load_and_gen_vllm.py'\n_src = _f.read_text()\n_src = _src.replace('enable_sleep_mode=True',     'enable_sleep_mode=False')\n_src = _src.replace('enable_chunked_prefill=True', 'enable_chunked_prefill=False')\n_f.write_text(_src)\nlg = importlib.reload(lg)\nlg.tqdm = lambda it, **kw: it\n\nfrom vllm import LLM\ntry:\n    _reuse = (isinstance(mllm, LLM)\n              and mllm.llm_engine.model_config.max_model_len == MAX_MODEL_LEN)\nexcept:\n    _reuse = False\n\nif not _reuse:\n    try:    mllm.shutdown()\n    except: pass\n    total_gib, free_gib = [float(x) / 1024 for x in subprocess.check_output(\n        'nvidia-smi --query-gpu=memory.total,memory.free --format=csv,noheader,nounits',\n        shell=True, text=True).strip().split(',')]\n    util = round(min(0.92, (free_gib - 4) / total_gib), 3)\n    mllm, sampling_params = lg.vllm_mllm_init(\n        str(MODEL_DIR), tp=1, gpu_memory_utilization=util, max_model_len=MAX_MODEL_LEN)\n    processor = AutoProcessor.from_pretrained(str(MODEL_DIR), trust_remote_code=True)\n\nsampling_params.seed = SEED\n\n# ── Dataset — resume from checkpoint if interrupted ───────────────────────\nproblem_dirs = sorted(GEO3K_DIR.iterdir(), key=lambda p: int(p.name))\n\nresults  = json.loads(RESULTS_FILE.read_text()) if RESULTS_FILE.exists() else []\ndone_ids = {r['id'] for r in results}\nremaining = [d for d in problem_dirs if str(d.name) not in done_ids]\nprint(f'{len(problem_dirs)} total | {len(done_ids)} done | {len(remaining)} remaining')\n\n# ── Helpers (identical to cell 8) ────────────────────────────────────────\ndef build_prompt(problem_text, choices):\n    lines = [f'Question: {problem_text}', 'Choices:']\n    for i, c in enumerate(choices):\n        lines.append(f'({chr(65+i)}) {c}')\n    lines.append('Put your final answer in \\\\boxed{}.')\n    return '\\n'.join(lines)\n\ndef extract_answer(text, choices):\n    m = re.search(r'\\\\boxed\\{([^}]*)\\}', text)\n    if m:\n        boxed = m.group(1).strip()\n        if re.fullmatch(r'[A-Da-d]', boxed):\n            return boxed.upper()\n        norm = lambda s: re.sub(r'\\s+', ' ', s.strip().upper())\n        for i, c in enumerate(choices):\n            if norm(boxed) == norm(c):\n                return chr(65 + i)\n    letters = re.findall(r'\\b([A-D])\\b', text[-400:])\n    return letters[-1].upper() if letters else None\n\n# ── Inference — text only; bypass multimodal processor to avoid None image ──\nfor i, prob_dir in enumerate(remaining, 1):\n    data     = json.loads((prob_dir / 'data.json').read_text())\n    question = build_prompt(data['problem_text'], data['choices'])\n\n    try:\n        text   = processor.apply_chat_template(\n            [{'role': 'user', 'content': question}],\n            tokenize=False, add_generation_prompt=True)\n        inputs = [{'prompt_token_ids': processor.tokenizer.encode(text, add_special_tokens=False)}]\n        output = mllm.generate(inputs, sampling_params=sampling_params, use_tqdm=False)[0].outputs[0]\n    except Exception:\n        print(f'[{i}/{len(remaining)}] ERROR {prob_dir.name}:\\n{traceback.format_exc()}')\n        continue\n\n    token_ids    = list(getattr(output, 'token_ids', []) or [])\n    predicted    = extract_answer(output.text or '', data['choices'])\n    ground_truth = str(data['answer']).strip().upper()\n    results.append({\n        'id':           str(prob_dir.name),\n        'problem_type': data.get('problem_type_graph', []),\n        'goal_type':    data.get('problem_type_goal', []),\n        'question':     question,\n        'output':       output.text or '',\n        'predicted':    predicted,\n        'ground_truth': ground_truth,\n        'correct':      predicted == ground_truth,\n        'used_latent':  151666 in token_ids or 151667 in token_ids,\n        'finish':       output.finish_reason,\n        'tokens':       len(token_ids),\n    })\n\n    if i % SAVE_EVERY == 0 or i == len(remaining):\n        RESULTS_FILE.write_text(json.dumps(results, indent=2))\n        n       = len(results)\n        correct = sum(r['correct'] for r in results)\n        print(f'[{i}/{len(remaining)}] saved={n} acc={correct/n:.3f}')\n\nn       = len(results)\ncorrect = sum(r['correct'] for r in results)\nprint(f'\\n=== Geometry3K No-Image Results (n={n}) ===')\nprint(f'accuracy: {correct}/{n} = {correct/max(n,1):.3f}')\nprint(f'results → {RESULTS_FILE}')\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "gwbfyy1zdyw",
   "source": "# 10) Geometry3K — logic form baseline (question + choices + diagram logic form, no image)\nimport importlib, json, os, re, subprocess, sys, traceback\nfrom pathlib import Path\nfrom transformers import AutoProcessor\n\nWORK          = Path('/content/work/multimodal_ml')\nMODEL_DIR     = Path('/content/work/models/Monet-7B')\nGEO3K_DIR     = WORK / 'datasets/geometry_3k/test'\nRESULTS_FILE  = WORK / 'results/geo3k_results_logic_form.json'\nMAX_MODEL_LEN = 32768\nSEED          = 0\nSAVE_EVERY    = 50\n\nos.environ.update({\n    'LATENT_SIZE': '10', 'LATENT_START_ID': '151666', 'LATENT_END_ID': '151667',\n    'VLLM_USE_V1': '1', 'VLLM_WORKER_MULTIPROC_METHOD': 'spawn', 'VLLM_ENABLE_V1_MULTIPROCESSING': '0',\n})\nRESULTS_FILE.parent.mkdir(parents=True, exist_ok=True)\n\nsys.path.insert(0, str(WORK / 'monet'))\nimport inference.apply_vllm_monet\nimport inference.load_and_gen_vllm as lg\n\n_f = WORK / 'monet/inference/load_and_gen_vllm.py'\n_src = _f.read_text()\n_src = _src.replace('enable_sleep_mode=True',     'enable_sleep_mode=False')\n_src = _src.replace('enable_chunked_prefill=True', 'enable_chunked_prefill=False')\n_f.write_text(_src)\nlg = importlib.reload(lg)\nlg.tqdm = lambda it, **kw: it\n\nfrom vllm import LLM\ntry:\n    _reuse = (isinstance(mllm, LLM)\n              and mllm.llm_engine.model_config.max_model_len == MAX_MODEL_LEN)\nexcept:\n    _reuse = False\n\nif not _reuse:\n    try:    mllm.shutdown()\n    except: pass\n    total_gib, free_gib = [float(x) / 1024 for x in subprocess.check_output(\n        'nvidia-smi --query-gpu=memory.total,memory.free --format=csv,noheader,nounits',\n        shell=True, text=True).strip().split(',')]\n    util = round(min(0.92, (free_gib - 4) / total_gib), 3)\n    mllm, sampling_params = lg.vllm_mllm_init(\n        str(MODEL_DIR), tp=1, gpu_memory_utilization=util, max_model_len=MAX_MODEL_LEN)\n    processor = AutoProcessor.from_pretrained(str(MODEL_DIR), trust_remote_code=True)\n\nsampling_params.seed = SEED\n\n# ── Dataset — resume from checkpoint if interrupted ───────────────────────\nproblem_dirs = sorted(GEO3K_DIR.iterdir(), key=lambda p: int(p.name))\n\nresults  = json.loads(RESULTS_FILE.read_text()) if RESULTS_FILE.exists() else []\ndone_ids = {r['id'] for r in results}\nremaining = [d for d in problem_dirs if str(d.name) not in done_ids]\nprint(f'{len(problem_dirs)} total | {len(done_ids)} done | {len(remaining)} remaining')\n\n# ── Helpers ───────────────────────────────────────────────────────────────\ndef build_prompt(problem_text, choices, diagram_logic_form):\n    \"\"\"Replaces the image with the structured diagram logic form.\"\"\"\n    lines = ['Diagram:']\n    lines.extend(diagram_logic_form)\n    lines.append(f'Question: {problem_text}')\n    lines.append('Choices:')\n    for i, c in enumerate(choices):\n        lines.append(f'({chr(65+i)}) {c}')\n    lines.append('Put your final answer in \\\\boxed{}.')\n    return '\\n'.join(lines)\n\ndef extract_answer(text, choices):\n    m = re.search(r'\\\\boxed\\{([^}]*)\\}', text)\n    if m:\n        boxed = m.group(1).strip()\n        if re.fullmatch(r'[A-Da-d]', boxed):\n            return boxed.upper()\n        norm = lambda s: re.sub(r'\\s+', ' ', s.strip().upper())\n        for i, c in enumerate(choices):\n            if norm(boxed) == norm(c):\n                return chr(65 + i)\n    letters = re.findall(r'\\b([A-D])\\b', text[-400:])\n    return letters[-1].upper() if letters else None\n\n# ── Inference — logic form as image substitute; bypass multimodal processor ──\nfor i, prob_dir in enumerate(remaining, 1):\n    data       = json.loads((prob_dir / 'data.json').read_text())\n    logic_form = json.loads((prob_dir / 'logic_form.json').read_text())\n    diagram    = logic_form.get('diagram_logic_form', [])\n    question   = build_prompt(data['problem_text'], data['choices'], diagram)\n\n    try:\n        text   = processor.apply_chat_template(\n            [{'role': 'user', 'content': question}],\n            tokenize=False, add_generation_prompt=True)\n        inputs = [{'prompt_token_ids': processor.tokenizer.encode(text, add_special_tokens=False)}]\n        output = mllm.generate(inputs, sampling_params=sampling_params, use_tqdm=False)[0].outputs[0]\n    except Exception:\n        print(f'[{i}/{len(remaining)}] ERROR {prob_dir.name}:\\n{traceback.format_exc()}')\n        continue\n\n    token_ids    = list(getattr(output, 'token_ids', []) or [])\n    predicted    = extract_answer(output.text or '', data['choices'])\n    ground_truth = str(data['answer']).strip().upper()\n    results.append({\n        'id':           str(prob_dir.name),\n        'problem_type': data.get('problem_type_graph', []),\n        'goal_type':    data.get('problem_type_goal', []),\n        'question':     question,\n        'output':       output.text or '',\n        'predicted':    predicted,\n        'ground_truth': ground_truth,\n        'correct':      predicted == ground_truth,\n        'used_latent':  151666 in token_ids or 151667 in token_ids,\n        'finish':       output.finish_reason,\n        'tokens':       len(token_ids),\n    })\n\n    if i % SAVE_EVERY == 0 or i == len(remaining):\n        RESULTS_FILE.write_text(json.dumps(results, indent=2))\n        n       = len(results)\n        correct = sum(r['correct'] for r in results)\n        print(f'[{i}/{len(remaining)}] saved={n} acc={correct/n:.3f}')\n\nn       = len(results)\ncorrect = sum(r['correct'] for r in results)\nprint(f'\\n=== Geometry3K Logic-Form Results (n={n}) ===')\nprint(f'accuracy: {correct}/{n} = {correct/max(n,1):.3f}')\nprint(f'results → {RESULTS_FILE}')\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}